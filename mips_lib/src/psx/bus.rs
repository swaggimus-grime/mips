use std::option::Option;
use std::cmp::min;
use std::ops::DerefMut;
use log::{info, warn};
use crate::error::MipsResult;
use crate::psx::addressable::{AccessWidth, Addressable};
use crate::psx::bios::bios::Bios;
use crate::psx::graphics::gpu::{Gpu, VideoStandard};
use crate::psx::memory::mem_ctrl::*;
use crate::psx::processor::{cpu, irq, ClockCycle};
use crate::psx::memory::ram::Ram;
use crate::psx::memory::scratch_pad::ScratchPad;
use crate::psx::processor::cop0::Cop0;
use crate::psx::sound::spu::Spu;
use crate::psx::sync::Synchronizer;
use crate::psx::{cd, mdec, pad_memcard, sync, timers, xmem};
use crate::psx::cd::disc;
use crate::psx::cd::disc::Disc;
use crate::psx::exe::Exe;
use crate::psx::graphics::gpu;
use crate::psx::graphics::rasterizer::handle::Frame;
use crate::psx::mdec::MDec;
use crate::psx::memory::dma::Dma;
use crate::psx::memory::{dma, map};
use crate::psx::pad_memcard::PadMemCard;
use crate::psx::processor::cpu::Cpu;
use crate::psx::processor::gte::Gte;
use crate::psx::sound::spu;
use crate::psx::timers::Timers;
use crate::psx::tty::Tty;
use crate::util::ds::box_slice::BoxSlice;

pub struct Bus {
    pub cpu: Cpu,
    pub cop0: Cop0,
    pub gte: Gte,
    pub xmem: xmem::XMemory,
    ram_size: u32,
    pub(crate) scratch_pad: ScratchPad,
    pub cycles: ClockCycle,
    /// Memory control registers
    mem_control: [u32; 9],
    cache_control: u32,
    pub irq: irq::InterruptState,
    pub sync: Synchronizer,
    pub dma: Dma,
    pub timers: Timers,
    pub gpu: Gpu,
    pub mdec: MDec,
    pub spu: Spu,
    pub cd: cd::CdInterface,
    pub pad_memcard: PadMemCard,
    /// Used to simulate the CPU slowdown generated by DMA operation
    dma_timing_penalty: ClockCycle,
    /// When this variable is `true` the CPU is stopped for DMA operation
    cpu_stalled_for_dma: bool,
    pub frame_done: bool,
    pub exe: Option<Exe>,
    tty: Tty
}

impl Bus {

    pub fn new(bios: Bios, cdc_firmware: [u8; cd::CDC_ROM_SIZE], disc: Option<disc::Disc>) -> MipsResult<Bus> {
        let cd = cd::CdInterface::new(disc, cdc_firmware)?;
        
        let mut xmem = xmem::XMemory::new();
        xmem.set_bios(bios.rom());

        Ok(Bus {
            cpu: Cpu::new(),
            cop0: Cop0::new(),
            gte: Gte::new(),
            xmem,
            ram_size: 0,
            scratch_pad: ScratchPad::new(),
            cycles: 0,
            mem_control: [0; 9],
            cache_control: 0,
            irq: irq::InterruptState::new(),
            sync: Synchronizer::new(),
            dma: Dma::new(),
            timers: Timers::new(),
            gpu: Gpu::new(VideoStandard::Ntsc),
            mdec: MDec::new(),
            spu: Spu::new(),
            cd,
            pad_memcard: PadMemCard::new(),
            dma_timing_penalty: 0,
            cpu_stalled_for_dma: false,
            frame_done: false,
            exe: None,
            tty: Tty::new(),
        })
    }

    pub fn insert_disc(&mut self, disc: Disc)  {
        self.gpu.reset(disc.region().video_standard());
        self.cd.load_disc(disc);
    }

    /// Returns true if the instruction cache is enabled in the CACHE_CONTROL register
    pub(crate) fn icache_enabled(&self) -> bool {
        self.cache_control & 0x800 != 0
    }

    /// Returns true if the cache is in "tag test mode"
    pub fn tag_test_mode(&self) -> bool {
        self.cache_control & 4 != 0
    }

    pub fn tick(&mut self, cycles: ClockCycle) {
        self.cycles += cycles;
    }

    pub fn update(&mut self) {
        self.frame_done = false;
        while !self.frame_done {
            if self.cpu_stalled_for_dma {
                // Fast forward to the next event
                self.cycles = self.sync.first_event();
            } else {
                while !sync::is_event_pending(self) {
                    cpu::run_next_instruction(self);
                }
            }

            sync::handle_events(self);
        }

        // Rebase the event counters relative to the cycle_counter to make sure they don't overflow
        sync::rebase_counters(self);
    }

    pub fn take_frame(&mut self) -> Option<Frame> {
        self.gpu.take_frame()
    }

    /// Get pending audio samples since the last call to `clear_audio_samples`
    pub fn get_audio_samples(&mut self) -> &[i16] {
        spu::get_samples(self)
    }

    /// Clear any pending audio samples. This must be called at least once per frame.
    pub fn clear_audio_samples(&mut self) {
        spu::clear_samples(self)
    }

    /// Execute a memory read and return the value alongside with the number of cycles necessary for
    /// the load to complete;
    pub(crate) fn load<T: Addressable>(&mut self, address: u32) -> T {
        let abs_addr = map::mask_region(address);

        // XXX Shouldn't we set dma_timing_penalty to 0 once we've "ticked" it? Mednafen doesn't do
        // it but I don't understand why not. Maybe it's just that the DMA is updated often enough
        // that it doesn't matter because the timing penalty is updated constantly?
        self.tick(self.dma_timing_penalty);

        if let Some(offset) = map::RAM.contains(abs_addr) {
            self.tick(3);
            return self.xmem.ram_load(offset);
        }

        if let Some(offset) = map::BIOS.contains(abs_addr) {
            // XXX Mednafen doesn't add any penalty for BIOS read, which sounds wrong. It's
            // probably not a common-enough occurence to matter
            return self.xmem.bios_load(offset);
        }

        if let Some(offset) = map::SPU.contains(abs_addr) {
            if T::width() == AccessWidth::Word {
                self.tick(36);
            } else {
                self.tick(16);
            }

            return spu::load(self, offset);
        }

        if let Some(offset) = map::DMA.contains(abs_addr) {
            self.tick(1);
            return dma::load(self, offset);
        }

        if let Some(offset) = map::TIMERS.contains(abs_addr) {
            self.tick(1);
            return timers::load(self, offset);
        }

        if let Some(offset) = map::GPU.contains(abs_addr) {
            self.tick(1);
            return gpu::load(self, offset);
        }

        if let Some(offset) = map::MDEC.contains(abs_addr) {
            self.tick(1);
            return mdec::load(self, offset);
        }

        if let Some(offset) = map::PAD_MEMCARD.contains(abs_addr) {
            self.tick(1);
            return pad_memcard::load(self, offset);
        }

        if let Some(offset) = map::CDROM.contains(abs_addr) {
            self.tick(6 * T::width() as i32);
            return cd::load(self, offset);
        }

        if let Some(off) = map::IRQ_CONTROL.contains(abs_addr) {
            self.tick(1);

            let v = match off {
                0 => u32::from(irq::status(self)),
                4 => u32::from(irq::mask(self)),
                _ => panic!("Unhandled IRQ load at address {:08x}", abs_addr),
            };

            // Since the IRQ registers are only 16bit wide the high 32bits are undefined. In
            // practice the high bits appear to maintain the value of the previous load.
            //
            // We could try to emulate this behaviour by keeping track of the previously loaded
            // value and use that but it's unclear if any game relies on this edge case.
            //
            // So why 0x1f80 in the high bits?
            //
            // In the BIOS IRQ handler just before loading the value of the mask the code loads the
            // base address of the IRQ handler (0x1f801070) then does an LW of the mask (base + 4).
            // In this case the load will return 0x1f80 in the high 16 bits.
            //
            // So any code that does "load IRQ register address -> load IRQ register" in sequence
            // will have 1f80 in the high bits, so it's a sane default.
            return Addressable::from_u32(v | 0x1f80_0000);
        }

        if map::EXPANSION_1.contains(abs_addr).is_some() {
            // No expansion implemented. Returns full ones when no
            // expansion is present
            return Addressable::from_u32(!0);
        }

        if map::CACHE_CONTROL.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled cache control access");
            }

            return Addressable::from_u32(self.cache_control);
        }

        if let Some(offset) = map::MEM_CONTROL.contains(abs_addr) {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled MEM_CONTROL {:?} access", T::width());
            }

            let index = (offset >> 2) as usize;

            return Addressable::from_u32(self.mem_control[index]);
        }

        if map::RAM_SIZE.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled RAM_SIZE access");
            }

            return Addressable::from_u32(self.ram_size);
        }

        if cfg!(feature = "debugger") {
            warn!("Unhandled load at address {:08x}", abs_addr);
            Addressable::from_u32(0xdeaddead)
        } else {
            panic!("Unhandled load at address {:08x}", abs_addr);
        }
    }

    /// Decode `address` and perform the store to the target module
    pub(crate) fn store<T: Addressable>(&mut self, address: u32, val: T) {
        let abs_addr = map::mask_region(address);

        if let Some(offset) = map::RAM.contains(abs_addr) {
            self.xmem.ram_store(offset, val);
            return;
        }

        if let Some(offset) = map::SCRATCH_PAD.contains(abs_addr) {
            return self.scratch_pad.store(offset, val);
        }

        if let Some(offset) = map::SPU.contains(abs_addr) {
            spu::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::DMA.contains(abs_addr) {
            dma::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::TIMERS.contains(abs_addr) {
            timers::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::GPU.contains(abs_addr) {
            gpu::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::MDEC.contains(abs_addr) {
            mdec::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::PAD_MEMCARD.contains(abs_addr) {
            pad_memcard::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::CDROM.contains(abs_addr) {
            cd::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::IRQ_CONTROL.contains(abs_addr) {
            match offset {
                0 => irq::ack(self, val.as_u16()),
                4 => irq::set_mask(self, val.as_u16()),
                _ => panic!("Unhandled IRQ store at address {:08x}", abs_addr),
            }

            return;
        }

        if let Some(offset) = map::EXPANSION_1.contains(abs_addr) {
            warn!("Unhandled write to expansion 1 register {:x}", offset);
            return;
        }

        if let Some(offset) = map::MEM_CONTROL.contains(abs_addr) {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled MEM_CONTROL {:?} access", T::width());
            }

            let val = val.as_u32();

            // We don't actually implement those registers, we assume that all BIOSes and games are
            // going to use the default memory configuration. I'm not aware of any game that breaks
            // this assumption. Still, we can catch any attempt at using a non-standard
            // configuration and report an error.
            match offset {
                // Expansion 1 base address
                0 => {
                    if val != 0x1f00_0000 {
                        panic!("Bad expansion 1 base address: 0x{:08x}", val);
                    }
                }
                // Expansion 2 base address
                4 => {
                    if val != 0x1f80_2000 {
                        panic!("Bad expansion 2 base address: 0x{:08x}", val);
                    }
                }
                _ => (),
            }

            let index = (offset >> 2) as usize;
            self.mem_control[index] = val;
            return;
        }

        if map::CACHE_CONTROL.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled cache control access");
            }

            self.cache_control = val.as_u32();

            return;
        }

        if map::RAM_SIZE.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled RAM_SIZE access");
            }

            self.ram_size = val.as_u32();
            return;
        }

        if let Some(offset) = map::EXPANSION_2.contains(abs_addr) {
            if offset == 0x23 || offset == 0x80 {
                self.tty.push_char(val.as_u8() as char);
            }
            else if offset == 0x41 || offset == 0x42 {
                let post_code = val.as_u32() & 0x0F;
                info!("BIOS POST status: {:x}", post_code);
                if post_code == 0x07 {
                    //TODO: sideload exe
                }
            }
            else if offset == 0x70 {
                info!("BIOS POST2 status: {:0x}", val.as_u32() & 0x0F);
            }
            else {
                warn!("Unhandled write to expansion 2 register {:x}", offset);
            }
            return;
        }

        panic!(
            "Unhandled store at address {:08x} (val=0x{:08x})",
            abs_addr,
            val.as_u32()
        );
    }

    pub fn set_dma_timing_penalty(&mut self, penalty: ClockCycle) {
        // XXX This is from mednafen and it's hacky. Basically since we store the `free_cycles` in
        // u8 we can't stall the CPU for more than 255 cycles or it'll overflow. The reason this
        // limitation at 200 doesn't break everything is because the DMA is refreshed very often
        // and it will call this method again with the remainder of the penalty as long as it
        // exists. It's still very ugly...
        self.dma_timing_penalty = min(penalty, 200);
    }

    pub fn set_cpu_stalled_for_dma(&mut self, stalled: bool) {
        self.cpu_stalled_for_dma = stalled;
    }
}
